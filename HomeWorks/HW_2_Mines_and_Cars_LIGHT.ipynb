{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_2_Mines_and_Cars_LIGHT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4mVxmFIwGn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF8IxWsQwDJR",
        "colab_type": "text"
      },
      "source": [
        "# Light."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPxSpAS5w64b",
        "colab_type": "text"
      },
      "source": [
        "### Задача."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlTarFMHw9mI",
        "colab_type": "text"
      },
      "source": [
        "Создайте модель для распознавания рукописных цифр из набора MNIST (можно воспользоваться ноутбуком 1-го занятия) и проведите ряд тестов: \n",
        "1. Запустите сеть с различными размерами обучающей и проверочной выборок: \n",
        "- Обучающая выборка 50.000 примеров \n",
        "- Обучающая выборка 10.000 примеров \n",
        "- Обучающая выборка 500 примеров \n",
        "2. Создайте еще два варианта сети и сравните значения точности на проверочной выборке (на \n",
        "последней эпохе) и на тестовой выборке. Сделайте сравнительную таблицу. \n",
        "3. Создайте сеть следующей архитектуры: \n",
        "- 4 Dense слоя \n",
        "- 3 Dropout слоя\n",
        "- 3 BatchNormalization слоя \n",
        "\n",
        "Напишите свои выводы по результатам проведенных тестов. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0mVkxt4C3fb",
        "colab_type": "text"
      },
      "source": [
        "### Решение."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fknfh6Hwfa6",
        "colab_type": "code",
        "outputId": "e67ba88f-cdf2-4f6b-e83b-736f6c97e957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# prepare data\n",
        "(x_train, y_train_org), (x_test, y_test_org) = mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train = x_train / 255\n",
        "x_test = x_test.astype('float32')\n",
        "x_test = x_test / 255\n",
        "y_train = utils.to_categorical(y_train_org, 10)\n",
        "y_test = utils.to_categorical(y_test_org, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWOyVOLkygm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# task 1\n",
        "list_sample = [50000, 10000, 500]\n",
        "string = 0\n",
        "table_vars = pd.DataFrame(columns = ['size_of_sample', 'dropout_layers', 'batch_norm_layers', 'accuracy_train', 'accuracy_test'])\n",
        "for sample in list_sample:\n",
        "  model = Sequential()\n",
        "  model.add(Dense(800, input_dim = 784, activation='relu'))\n",
        "  model.add(Dense(100, activation='linear'))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  history = model.fit(x_train[:sample+1], y_train[:sample+1], batch_size=768, epochs=30, verbose=1)\n",
        "  res_base = model.evaluate(x_test, y_test, verbose=1)\n",
        "  table_vars.loc[string] = [sample, 0, 0, history.history['accuracy'][-1], res_base[1]]\n",
        "  string += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoivuapC5VGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# task 2\n",
        "# network with dropout layers\n",
        "model = Sequential()\n",
        "model.add(Dropout(0.3, input_shape=(784,)))\n",
        "model.add(Dense(800, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(100, activation='linear'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(x_train[:50001], y_train[:50001], batch_size=768, epochs=30, verbose=1)\n",
        "# I left 50,000 for equal conditions \n",
        "res_base = model.evaluate(x_test, y_test, verbose=1)\n",
        "table_vars.loc[string] = [50000, 3, 0, history.history['accuracy'][-1], res_base[1]]\n",
        "string += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61cyKXAw5_3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# network with batchnormalization layers\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(784, )))\n",
        "model.add(Dense(800, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(100, activation='linear'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(100, activation='linear'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(x_train[:50001], y_train[:50001], batch_size=768, epochs=30, verbose=1)\n",
        "res_base = model.evaluate(x_test, y_test, verbose=1)\n",
        "table_vars.loc[string] = [50000, 3, 3, history.history['accuracy'][-1], res_base[1]]\n",
        "string += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoXhzP-G7NRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# task 3\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(784, )))\n",
        "model.add(Dense(800, input_dim = 784, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(100, activation='linear'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(100, activation='linear'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(x_train[:50001], y_train[:50001], batch_size=768, epochs=30, verbose=1)\n",
        "res_base = model.evaluate(x_test, y_test, verbose=1)\n",
        "table_vars.loc[string] = [50000, 0, 3, history.history['accuracy'][-1], res_base[1]]\n",
        "string += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvYC_9676OMt",
        "colab_type": "code",
        "outputId": "84aa0e9d-ff99-4d27-8740-ee9da7f44462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "table_vars"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>size_of_sample</th>\n",
              "      <th>dropout_layers</th>\n",
              "      <th>batch_norm_layers</th>\n",
              "      <th>accuracy_train</th>\n",
              "      <th>accuracy_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.99924</td>\n",
              "      <td>0.9768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.99940</td>\n",
              "      <td>0.9568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>500.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.8452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.98550</td>\n",
              "      <td>0.9836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.99950</td>\n",
              "      <td>0.9788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>50000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.99640</td>\n",
              "      <td>0.9803</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   size_of_sample  dropout_layers  ...  accuracy_train  accuracy_test\n",
              "0         50000.0             0.0  ...         0.99924         0.9768\n",
              "1         10000.0             0.0  ...         0.99940         0.9568\n",
              "2           500.0             0.0  ...         1.00000         0.8452\n",
              "3         50000.0             3.0  ...         0.98550         0.9836\n",
              "4         50000.0             0.0  ...         0.99950         0.9788\n",
              "5         50000.0             3.0  ...         0.99640         0.9803\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq8cHLFc6Y5J",
        "colab_type": "text"
      },
      "source": [
        "### Выводы.\n",
        "\n",
        "Чем меньше тестовая выборка, тем быстрее мы достигаем высоких результатов на тренировочной выборке и тем хуже результаты на проверочной (тестовой) выборке. Нужно стараться использовать как можно больше данных, но следить чтобы они были сбалансированны, очищенны и хорошо подготовленны.\n",
        "\n",
        "drop_out показал себя просто отлично. Он конечно снизил показатель на обучаемой выборке, но это говорит только об избавлении от переобучении. А вот на тестовой выборке очень существенный прирост. Результаты прошлой недели можно было бы улучшить.\n",
        "\n",
        "batch_normalization также дал улучшение, но не такое существенное. Их общее использование к моему удивлению также дало хороший результат. Хотя я полагал это будет слишком бессмысленно сложная нейросеть. \n",
        "\n",
        "Хотя у меня лучший показатель дал чистый дропаут, для улучшения показателей я бы продолжил эксперименты и с комбинацией этих методов, и с изменением гиперпараметров drop_out. Однако меня ждут мины в PRO задании))"
      ]
    }
  ]
}